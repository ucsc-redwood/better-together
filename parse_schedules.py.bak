#!/usr/bin/env python3
"""
Parse schedule log files and generate averaged statistics per schedule.

Usage:
    python parse_schedules.py /path/to/log/files [--model /path/to/model.json]
"""

import os
import re
import sys
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Parse schedule log files")
    parser.add_argument(
        "input", help="Path to log file or directory containing log files"
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Print detailed statistics for each log file",
    )
    parser.add_argument(
        "--model",
        "-m",
        help="Path to JSON file containing model predictions",
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output directory for visualization files",
        default="./results",
    )
    parser.add_argument(
        "--start-time",
        type=float,
        help="Start time as percentage (0.0-1.0) of overall execution",
        default=0.0,
    )
    parser.add_argument(
        "--end-time",
        type=float,
        help="End time as percentage (0.0-1.0) of overall execution",
        default=1.0,
    )
    return parser.parse_args()


def find_log_files(input_path):
    """Find all log files matching the pattern in the specified directory or return the input file if it's a file."""
    log_files = []

    # Check if input is a file or directory
    if os.path.isfile(input_path):
        # Check if the file matches our pattern
        filename = os.path.basename(input_path)
        if re.match(r"^[^_]+_[^_]+_schedules_\d+\.log$", filename):
            log_files.append(input_path)
            print(f"Using log file: {input_path}")
        else:
            print(
                f"Warning: File {input_path} doesn't match expected pattern for log files"
            )
            log_files.append(input_path)  # Include it anyway
    elif os.path.isdir(input_path):
        # It's a directory, search for matching files
        pattern = re.compile(r"^[^_]+_[^_]+_schedules_\d+\.log$")
        try:
            for filename in os.listdir(input_path):
                if pattern.match(filename):
                    log_files.append(os.path.join(input_path, filename))
        except Exception as e:
            print(f"Error searching directory {input_path}: {e}")
            return []

        print(f"Found {len(log_files)} log files in {input_path}")
    else:
        print(f"Error: {input_path} is neither a file nor a directory")
        return []

    return log_files


def extract_python_sections(content):
    """Extract all Python sections between '### Python Begin ###' and '### Python End ###'."""
    python_sections = re.findall(
        r"### Python Begin ###(.*?)### Python End ###", content, re.DOTALL
    )
    return python_sections


def extract_schedule_uid(section):
    """Extract Schedule_UID from a Python section."""
    uid_match = re.search(r"Schedule_UID=([A-Za-z0-9\-]+)", section)
    if uid_match:
        return uid_match.group(1)
    return None


def extract_frequency(section):
    """Extract frequency information from a Python section."""
    freq_match = re.search(r"Frequency=(\d+) Hz", section)
    if freq_match:
        return int(freq_match.group(1))
    return 24576000  # Default frequency in Hz


def parse_task_data(section):
    """Parse task data from a Python section."""
    tasks = {}
    pattern = r"Task=(\d+) Chunk=(\d+) Start=(\d+) End=(\d+) Duration=(\d+)"
    task_matches = re.findall(pattern, section)

    for match in task_matches:
        task_id = int(match[0])
        chunk_id = int(match[1])
        start = int(match[2])
        end = int(match[3])
        duration = int(match[4])

        if task_id not in tasks:
            tasks[task_id] = {}

        tasks[task_id][chunk_id] = {
            "start": start,
            "end": end,
            "duration_cycles": duration,
        }

    return tasks


def process_log_file(log_file):
    """Process a single log file and extract all schedule data."""
    print(f"Processing {log_file}...")
    schedules_data = []

    try:
        with open(log_file, "r") as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading {log_file}: {e}")
        return []

    # Extract device and application name from filename
    filename = os.path.basename(log_file)
    parts = filename.split("_")
    if len(parts) >= 2:
        device = parts[0]
        application = parts[1]
    else:
        device = "unknown"
        application = "unknown"

    # Extract all Python sections
    python_sections = extract_python_sections(content)
    print(f"Found {len(python_sections)} schedule sections in {log_file}")

    # Process each Python section
    for section_idx, section in enumerate(python_sections):
        # Extract schedule information
        schedule_uid = extract_schedule_uid(section)
        if not schedule_uid:
            print(f"Warning: Could not find Schedule_UID in section {section_idx+1}")
            continue

        frequency = extract_frequency(section)
        tasks = parse_task_data(section)

        # Calculate cycles to ms conversion factor
        cycles_to_ms = 1e3 / frequency

        # Calculate additional metrics per task and chunk
        task_metrics = {}
        chunk_metrics = defaultdict(lambda: {"total_duration": 0, "task_count": 0})

        # Calculate total end-to-end time for this schedule
        min_start_time = float('inf')
        max_end_time = 0

        for task_id, chunks in tasks.items():
            task_total_duration = 0
            task_metrics[task_id] = {"chunks": {}}

            # Find minimum start time and maximum end time
            for chunk_id, chunk_data in chunks.items():
                start_time = chunk_data["start"]
                end_time = chunk_data["end"]

                if start_time < min_start_time:
                    min_start_time = start_time
                if end_time > max_end_time:
                    max_end_time = end_time

                # Process chunk data
                duration_ms = chunk_data["duration_cycles"] * cycles_to_ms

                # Update task metrics
                task_metrics[task_id]["chunks"][chunk_id] = {
                    "start_ms": start_time * cycles_to_ms,
                    "end_ms": end_time * cycles_to_ms,
                    "duration_ms": duration_ms,
                }
                task_total_duration += duration_ms

                # Update chunk metrics
                chunk_metrics[chunk_id]["total_duration"] += duration_ms
                chunk_metrics[chunk_id]["task_count"] += 1

            task_metrics[task_id]["total_duration_ms"] = task_total_duration

        # Calculate average duration per chunk
        for chunk_id, metrics in chunk_metrics.items():
            if metrics["task_count"] > 0:
                metrics["avg_duration"] = (
                    metrics["total_duration"] / metrics["task_count"]
                )
            else:
                metrics["avg_duration"] = 0

        # Calculate total execution time of this schedule
        total_execution_time_ms = (max_end_time - min_start_time) * cycles_to_ms if min_start_time != float('inf') else 0

        # Create schedule data
        schedule_data = {
            "device": device,
            "application": application,
            "schedule_uid": schedule_uid,
            "frequency_hz": frequency,
            "tasks": task_metrics,
            "chunks": dict(chunk_metrics),  # Convert defaultdict to dict
            "num_tasks": len(task_metrics),
            "num_chunks": len(chunk_metrics),
            "log_file": log_file,
            "min_start_time": min_start_time,
            "max_end_time": max_end_time,
            "total_execution_time_ms": total_execution_time_ms,
        }

        schedules_data.append(schedule_data)

    return schedules_data


def print_individual_statistics(schedules_data):
    """Print statistics for each schedule in each log file."""
    print("\n===== INDIVIDUAL SCHEDULE STATISTICS =====")

    for i, schedule in enumerate(schedules_data):
        device = schedule["device"]
        application = schedule["application"]
        schedule_uid = schedule["schedule_uid"]
        log_file = os.path.basename(schedule["log_file"])

        # Calculate total time across all tasks and chunks
        total_time_ms = 0
        for task_id, task_data in schedule["tasks"].items():
            total_time_ms += task_data["total_duration_ms"]

        print(f"\nSchedule {i+1}: {schedule_uid} (from {log_file})")
        print(f"Device: {device}, Application: {application}")
        print(f"Total time: {total_time_ms:.2f} ms")

        # Print average time by chunks
        print("Average time by chunks:")
        for chunk_id, chunk_data in sorted(schedule["chunks"].items()):
            avg_duration = chunk_data["avg_duration"]
            task_count = chunk_data["task_count"]
            total_duration = chunk_data["total_duration"]
            print(
                f"  Chunk {chunk_id}: {avg_duration:.2f} ms (avg) / {total_duration:.2f} ms (total) / {task_count} tasks"
            )

        print("-" * 50)


def create_comparison_visualization(
    avg_times, model_predictions, output_dir, raw_data, window_str=""
):
    """Create visualization comparing measured results with model predictions."""
    if not model_predictions or not avg_times:
        return

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Collect data for visualization
    schedule_uids = []
    measured_times = []
    predicted_times = []
    error_bars = []

    # Matching UIDs that have both measurements and predictions
    matching_uids = sorted(set(avg_times.keys()) & set(model_predictions.keys()))

    if not matching_uids:
        print("No matching UIDs found between measurements and predictions")
        return

    # Collect data
    for uid in matching_uids:
        schedule_uids.append(uid)
        measured_times.append(avg_times[uid]["avg_time_ms"])
        predicted_times.append(model_predictions[uid])

        # Calculate standard deviation for error bars from raw data
        std_dev = 0
        if uid in raw_data:
            # Calculate std dev for the total execution times
            if "execution_times_ms" in avg_times[uid]:
                execution_times = avg_times[uid]["execution_times_ms"]
                if len(execution_times) > 1:
                    std_dev = np.std(execution_times)

        error_bars.append(std_dev)

    # Convert to numpy arrays for easier manipulation
    measured_times = np.array(measured_times)
    predicted_times = np.array(predicted_times)
    error_bars = np.array(error_bars)

    # Create figure
    plt.figure(figsize=(14, 8))

    # Calculate positions for bars
    x = np.arange(len(schedule_uids))
    width = 0.35

    # Plot bars
    measured_bars = plt.bar(
        x - width / 2, measured_times, width, label="Measured", alpha=0.7
    )
    predicted_bars = plt.bar(
        x + width / 2, predicted_times, width, label="Predicted", alpha=0.7
    )

    # Add error bars to measured data
    plt.errorbar(
        x - width / 2,
        measured_times,
        yerr=error_bars,
        fmt="none",
        ecolor="black",
        capsize=5,
    )

    # Add labels and title
    plt.xlabel("Schedule UID")
    plt.ylabel("Time (ms)")
    title = "Comparison of Measured vs Predicted Execution Times"
    if window_str:
        title += f" ({window_str})"
    plt.title(title)
    plt.xticks(x, [uid.split("-")[1] for uid in schedule_uids], rotation=45, ha="right")
    plt.legend()

    # Add value labels on the bars
    def add_labels(bars):
        for bar in bars:
            height = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2.0,
                height + 0.1,
                f"{height:.2f}",
                ha="center",
                va="bottom",
                fontsize=8,
            )

    add_labels(measured_bars)
    add_labels(predicted_bars)

    # Add grid and adjust layout
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.tight_layout()

    # Save figure
    filename = "comparison_chart"
    if window_str:
        filename += f"_{window_str.replace(' ', '_')}"
    filename += ".png"

    plt.savefig(os.path.join(output_dir, filename), dpi=300)
    print(f"Visualization saved to {os.path.join(output_dir, filename)}")

    # Create scatter plot for correlation
    plt.figure(figsize=(10, 8))
    plt.scatter(predicted_times, measured_times, alpha=0.7)

    # Add diagonal line (perfect prediction)
    max_val = max(np.max(predicted_times), np.max(measured_times)) * 1.1
    plt.plot([0, max_val], [0, max_val], "r--", label="Perfect Prediction")

    # Add labels
    plt.xlabel("Predicted Time (ms)")
    plt.ylabel("Measured Time (ms)")
    title = "Correlation between Predicted and Measured Times"
    if window_str:
        title += f" ({window_str})"
    plt.title(title)

    # Add schedule labels to points
    for i, uid in enumerate(schedule_uids):
        plt.annotate(
            uid.split("-")[1],
            (predicted_times[i], measured_times[i]),
            textcoords="offset points",
            xytext=(0, 5),
            ha="center",
            fontsize=8,
        )

    # Calculate correlation coefficient
    correlation = np.corrcoef(predicted_times, measured_times)[0, 1]
    plt.text(
        0.05,
        0.95,
        f"Correlation: {correlation:.4f}",
        transform=plt.gca().transAxes,
        fontsize=12,
        verticalalignment="top",
    )

    plt.grid(True, linestyle="--", alpha=0.7)
    plt.legend()
    plt.tight_layout()

    # Save scatter plot
    scatter_filename = "correlation_plot"
    if window_str:
        scatter_filename += f"_{window_str.replace(' ', '_')}"
    scatter_filename += ".png"

    plt.savefig(os.path.join(output_dir, scatter_filename), dpi=300)
    print(f"Correlation plot saved to {os.path.join(output_dir, scatter_filename)}")


def perform_statistical_analysis(avg_times, model_predictions, window_str=""):
    """Perform detailed statistical analysis on measured vs predicted times."""
    if not model_predictions or not avg_times:
        return

    # Extract data for matched UIDs
    matching_uids = sorted(set(avg_times.keys()) & set(model_predictions.keys()))

    if not matching_uids:
        print("No matching UIDs found between measurements and predictions")
        return

    # Collect data
    measured_times = []
    predicted_times = []
    abs_differences = []
    rel_differences_pct = []

    for uid in matching_uids:
        measured = avg_times[uid]["avg_time_ms"]
        predicted = model_predictions[uid]

        measured_times.append(measured)
        predicted_times.append(predicted)

        # Calculate differences
        abs_diff = measured - predicted
        abs_differences.append(abs_diff)

        # Calculate relative difference as percentage
        if predicted != 0:
            rel_diff_pct = (abs_diff / predicted) * 100
        else:
            rel_diff_pct = float("inf")
        rel_differences_pct.append(rel_diff_pct)

    # Convert to numpy arrays
    measured_times = np.array(measured_times)
    predicted_times = np.array(predicted_times)
    abs_differences = np.array(abs_differences)
    rel_differences_pct = np.array([d for d in rel_differences_pct if not np.isinf(d)])

    # Calculate basic statistics
    correlation = np.corrcoef(measured_times, predicted_times)[0, 1]
    r_squared = correlation**2

    mse = np.mean(abs_differences**2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(abs_differences))

    # Check for under/over prediction bias
    under_predictions = sum(
        measured > predicted
        for measured, predicted in zip(measured_times, predicted_times)
    )
    over_predictions = sum(
        measured < predicted
        for measured, predicted in zip(measured_times, predicted_times)
    )
    exact_matches = sum(
        measured == predicted
        for measured, predicted in zip(measured_times, predicted_times)
    )

    # Count predictions within error margins
    within_5_pct = sum(abs(diff) <= 5 for diff in rel_differences_pct)
    within_10_pct = sum(abs(diff) <= 10 for diff in rel_differences_pct)
    within_20_pct = sum(abs(diff) <= 20 for diff in rel_differences_pct)

    # Print the analysis
    print(f"\n===== STATISTICAL ANALYSIS {window_str} =====")
    print(f"Total comparisons: {len(matching_uids)}")

    print("\nCorrelation Statistics:")
    print(f"Pearson correlation coefficient: {correlation:.4f}")
    print(f"Coefficient of determination (R²): {r_squared:.4f}")

    print("\nError Metrics:")
    print(f"Mean Squared Error (MSE): {mse:.4f} ms²")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f} ms")
    print(f"Mean Absolute Error (MAE): {mae:.4f} ms")

    print("\nError Distribution:")
    if len(rel_differences_pct) > 0:
        print(f"Mean percentage error: {np.mean(rel_differences_pct):.2f}%")
        print(f"Median percentage error: {np.median(rel_differences_pct):.2f}%")
        print(
            f"Standard deviation of percentage error: {np.std(rel_differences_pct):.2f}%"
        )
        print(f"Min percentage error: {np.min(rel_differences_pct):.2f}%")
        print(f"Max percentage error: {np.max(rel_differences_pct):.2f}%")

    print("\nPrediction Accuracy:")
    print(
        f"Within 5% margin: {within_5_pct} ({within_5_pct/len(rel_differences_pct)*100:.2f}% of valid comparisons)"
    )
    print(
        f"Within 10% margin: {within_10_pct} ({within_10_pct/len(rel_differences_pct)*100:.2f}% of valid comparisons)"
    )
    print(
        f"Within 20% margin: {within_20_pct} ({within_20_pct/len(rel_differences_pct)*100:.2f}% of valid comparisons)"
    )

    print("\nPrediction Bias:")
    print(
        f"Under-predictions (measured > predicted): {under_predictions} ({under_predictions/len(matching_uids)*100:.2f}%)"
    )
    print(
        f"Over-predictions (measured < predicted): {over_predictions} ({over_predictions/len(matching_uids)*100:.2f}%)"
    )
    print(
        f"Exact matches: {exact_matches} ({exact_matches/len(matching_uids)*100:.2f}%)"
    )

    # Return the metrics for potential further use
    return {
        "correlation": correlation,
        "r_squared": r_squared,
        "rmse": rmse,
        "mae": mae,
        "under_predictions": under_predictions,
        "over_predictions": over_predictions,
    }


def load_model_predictions(json_file_path):
    """Load model predictions from a JSON file."""
    if not os.path.exists(json_file_path):
        print(f"Error: Model file {json_file_path} not found")
        return {}
    
    try:
        with open(json_file_path, "r") as f:
            model_data = json.load(f)
    except Exception as e:
        print(f"Error loading model file {json_file_path}: {e}")
        return {}
    
    # Create a dictionary mapping schedule UIDs to their predicted times
    predictions = {}
    for schedule in model_data:
        if (
            "uid" in schedule
            and "metrics" in schedule
            and "max_time" in schedule["metrics"]
        ):
            uid = schedule["uid"]
            predicted_time = schedule["metrics"]["max_time"]
            predictions[uid] = predicted_time
    
    print(f"Loaded {len(predictions)} model predictions from {json_file_path}")
    return predictions


def calculate_average_execution_times(all_schedules, start_time_pct=0.0, end_time_pct=1.0):
    """Calculate average end-to-end execution times within a specified time window."""
    # Group schedules by their UID
    grouped_schedules = defaultdict(list)
    for schedule in all_schedules:
        grouped_schedules[schedule["schedule_uid"]].append(schedule)

    # Calculate aggregated execution times for each schedule UID
    avg_execution_times = {}

    for schedule_uid, schedules in grouped_schedules.items():
        execution_times_ms = []
        total_num_tasks = 0

        for schedule in schedules:
            # Skip if no execution time data
            if "min_start_time" not in schedule or "max_end_time" not in schedule:
                continue

            min_start = schedule["min_start_time"]
            max_end = schedule["max_end_time"]

            if min_start == float('inf') or max_end == 0:
                continue

            # Calculate the time window
            total_time_span = max_end - min_start
            window_start = min_start + (start_time_pct * total_time_span)
            window_end = min_start + (end_time_pct * total_time_span)

            # Count tasks in the time window
            tasks_in_window = 0
            total_time_in_window = 0

            for task_id, task_data in schedule["tasks"].items():
                for chunk_id, chunk_data in task_data["chunks"].items():
                    # Convert to cycles for comparison
                    chunk_start = chunk_data["start_ms"] / (1e3 / schedule["frequency_hz"])
                    chunk_end = chunk_data["end_ms"] / (1e3 / schedule["frequency_hz"])

                    # Check if this chunk is within the time window
                    if chunk_end >= window_start and chunk_start <= window_end:
                        tasks_in_window += 1

                        # Calculate the overlapping time
                        overlap_start = max(chunk_start, window_start)
                        overlap_end = min(chunk_end, window_end)
                        overlap_duration = overlap_end - overlap_start

                        # Convert back to ms
                        overlap_duration_ms = overlap_duration * (1e3 / schedule["frequency_hz"])
                        total_time_in_window += overlap_duration_ms

            if tasks_in_window > 0:
                avg_time = total_time_in_window / tasks_in_window
                execution_times_ms.append(avg_time)
                total_num_tasks += tasks_in_window

        # Calculate average across all log files
        if execution_times_ms:
            avg_execution_times[schedule_uid] = {
                "avg_time_ms": sum(execution_times_ms) / len(execution_times_ms),
                "execution_times_ms": execution_times_ms,
                "num_samples": len(execution_times_ms),
                "total_tasks": total_num_tasks
            }

    return avg_execution_times


def print_average_execution_times(avg_times, window_str=""):
    """Print the average execution times for each schedule."""
    print(f"\n===== AVERAGE EXECUTION TIMES {window_str} =====")
    print("Schedule UID                    : Avg Time (ms)  Samples  Total Tasks")
    print("-" * 70)

    for schedule_uid, data in sorted(avg_times.items()):
        print(
            f"{schedule_uid:30} : {data['avg_time_ms']:12.2f}  {data['num_samples']:7}  {data['total_tasks']:10}"
        )


def main():
    """Main function to process all log files."""
    args = parse_arguments()

    # Validate percentage inputs
    if args.start_time < 0.0 or args.start_time > 1.0:
        print("Error: start-time must be between 0.0 and 1.0")
        return 1
    if args.end_time < 0.0 or args.end_time > 1.0:
        print("Error: end-time must be between 0.0 and 1.0")
        return 1
    if args.end_time <= args.start_time:
        print("Error: end-time must be greater than start-time")
        return 1

    # Create window string for output
    window_str = f"Window {args.start_time:.2f}-{args.end_time:.2f}"

    # Load model predictions if specified
    model_predictions = {}
    if args.model:
        model_predictions = load_model_predictions(args.model)

    # Find all log files in the specified folder or use the specified file
    log_files = find_log_files(args.input)
    if not log_files:
        print(f"No log files found at {args.input}")
        return 1

    # Process each log file
    all_schedules = []

    for log_file in log_files:
        schedules_data = process_log_file(log_file)
        all_schedules.extend(schedules_data)

    if not all_schedules:
        print("No schedule data was found in any of the log files.")
        return 1

    # Print individual statistics if verbose mode is enabled
    if args.verbose:
        print_individual_statistics(all_schedules)

    # Calculate and print average execution times
    avg_times = calculate_average_execution_times(
        all_schedules, args.start_time, args.end_time
    )
    print_average_execution_times(avg_times, window_str)

    # Group schedules for error bar calculation
    raw_data_by_uid = defaultdict(list)
    for schedule in all_schedules:
        raw_data_by_uid[schedule["schedule_uid"]].append(schedule)

    # Compare with model predictions if available
    if model_predictions:
        print("\n===== MEASURED VS PREDICTED TIMES =====")
        print(
            "Schedule UID                    : Measured (ms)  Predicted (ms)  Difference (%)  "
        )
        print("-" * 80)

        for schedule_uid, data in sorted(avg_times.items()):
            measured_time = data["avg_time_ms"]

            if schedule_uid in model_predictions:
                predicted_time = model_predictions[schedule_uid]
                difference = measured_time - predicted_time
                diff_percent = (
                    (difference / predicted_time) * 100
                    if predicted_time != 0
                    else float("inf")
                )

                print(
                    f"{schedule_uid:30} : {measured_time:12.2f}  {predicted_time:14.2f}  {diff_percent:+14.2f}%"
                )
            else:
                print(f"{schedule_uid:30} : {measured_time:12.2f}  {'N/A':14}  {'N/A':14}")

        # Perform statistical analysis
        perform_statistical_analysis(avg_times, model_predictions, window_str)

        # Create visualization
        create_comparison_visualization(
            avg_times, model_predictions, args.output, raw_data_by_uid, window_str
        )

    print(
        f"\nProcessed {len(log_files)} log files with a total of {len(all_schedules)} schedule instances"
    )
    return 0


if __name__ == "__main__":
    sys.exit(main())
